{"cells":[{"cell_type":"code","execution_count":null,"id":"629fc2ae","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87864,"status":"ok","timestamp":1683296593202,"user":{"displayName":"Eric Barre","userId":"02191998857167728142"},"user_tz":240},"id":"629fc2ae","outputId":"57deabc2-2a7c-4d43-ac7d-bff77532e16e"},"outputs":[],"source":["%pip install --q squarify -U\n","%pip install --q plotly -U\n","%pip install --q matplotlib -U \n","%pip install --q scikit-learn -U\n","%pip install --q vectice -U"]},{"cell_type":"code","execution_count":null,"id":"84871ed3","metadata":{},"outputs":[],"source":["phs_id = \"PHA-1594\""]},{"cell_type":"code","execution_count":null,"id":"6e490e5d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2357,"status":"ok","timestamp":1683296624670,"user":{"displayName":"Eric Barre","userId":"02191998857167728142"},"user_tz":240},"id":"6e490e5d","outputId":"50ab8e2e-b381-4dd1-837c-b7e101c8ca3e"},"outputs":[],"source":["import vectice as vct\n","vec = vct.connect(config=\"token_e.json\")"]},{"attachments":{},"cell_type":"markdown","id":"509397bd","metadata":{"id":"509397bd"},"source":["## Import packages"]},{"cell_type":"code","execution_count":null,"id":"2f7f042e","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"elapsed":1857,"status":"ok","timestamp":1683296626516,"user":{"displayName":"Eric Barre","userId":"02191998857167728142"},"user_tz":240},"id":"2f7f042e","outputId":"a50e4bd2-a809-45e0-b93c-1a139f856a37","papermill":{"duration":0.098471,"end_time":"2022-01-15T09:50:45.169883","exception":false,"start_time":"2022-01-15T09:50:45.071412","status":"completed"},"tags":[]},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","# Importing the relevant libraries\n","import IPython.display\n","%matplotlib inline\n","import plotly.offline as py\n","py.init_notebook_mode(connected=True)\n","import plotly.graph_objs as go\n","import plotly.tools as tls\n","from matplotlib import pyplot as plt\n","import os\n","# D3 modules\n","from IPython.display import display\n","import datetime as dt\n","# sklearn\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.base import BaseEstimator, TransformerMixin\n"]},{"attachments":{},"cell_type":"markdown","id":"6baac016","metadata":{"id":"6baac016"},"source":["## Reading the data\n","\n","The dataset used in this project can be found here:<br>\n","* [items.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/items.csv)<br>\n","* [holidays_events.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/holidays_events.csv)<br>\n","* [stores.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/stores.csv)<br>\n","* [oil.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/oil.csv)<br>\n","* [transactions.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/transactions.csv)<br>\n","* [train_reduced.csv](https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/train_reduced.csv)\n","\n","Excute the cell below to download the files locally"]},{"cell_type":"code","execution_count":null,"id":"ae8739da","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2349,"status":"ok","timestamp":1683296628858,"user":{"displayName":"Eric Barre","userId":"02191998857167728142"},"user_tz":240},"id":"ae8739da","outputId":"8e2c6011-a425-4188-9993-5fab76285ad2"},"outputs":[],"source":["# Download the files locally\n","#!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/items.csv -q --no-check-certificate\n","#!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/holidays_events.csv -q --no-check-certificate\n","#!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/stores.csv -q --no-check-certificate\n","#!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/oil.csv -q --no-check-certificate\n","#!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/transactions.csv -q --no-check-certificate\n","#!wget https://vectice-examples.s3.us-west-1.amazonaws.com/Tutorial/ForecastTutorial/train_reduced.csv -q --no-check-certificate"]},{"attachments":{},"cell_type":"markdown","id":"a33948b8","metadata":{"id":"a33948b8"},"source":["#### Great! Let's build dataframes from the file for later use"]},{"cell_type":"code","execution_count":null,"id":"3e49aa30","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3403,"status":"ok","timestamp":1683296632257,"user":{"displayName":"Eric Barre","userId":"02191998857167728142"},"user_tz":240},"id":"3e49aa30","outputId":"e8229710-2741-4c20-dc09-adce3eaa1316"},"outputs":[],"source":["dtypes = {'store_nbr': np.dtype('int64'),\n","          'item_nbr': np.dtype('int64'),\n","          'unit_sales': np.dtype('float64'),\n","          'onpromotion': np.dtype('O')}\n","\n","items = pd.read_csv(\"items.csv\")\n","holiday_events = pd.read_csv(\"holidays_events.csv\", parse_dates=['date'])\n","stores = pd.read_csv(\"stores.csv\")\n","oil = pd.read_csv(\"oil.csv\", parse_dates=['date'])\n","transactions = pd.read_csv(\"transactions.csv\", parse_dates=['date'])\n","train = pd.read_csv(\"train_reduced.csv\", parse_dates=['date'], on_bad_lines='warn')"]},{"attachments":{},"cell_type":"markdown","id":"852f9e1c","metadata":{"id":"852f9e1c","papermill":{"duration":0.167189,"end_time":"2022-01-15T09:54:03.229848","exception":false,"start_time":"2022-01-15T09:54:03.062659","status":"completed"},"tags":[]},"source":["# Feature engineering"]},{"attachments":{},"cell_type":"markdown","id":"7fe02bb5","metadata":{"id":"7fe02bb5","papermill":{"duration":0.167783,"end_time":"2022-01-15T09:54:03.562802","exception":false,"start_time":"2022-01-15T09:54:03.395019","status":"completed"},"tags":[]},"source":["**Here we analyze the data and select the features for our model to be trained on.**"]},{"attachments":{},"cell_type":"markdown","id":"3a8bb667","metadata":{"id":"3a8bb667","papermill":{"duration":0.165965,"end_time":"2022-01-15T09:54:03.894763","exception":false,"start_time":"2022-01-15T09:54:03.728798","status":"completed"},"tags":[]},"source":["**Train**\n","id, date, store_nbr, item_nbr, unit_scale, on_promotion\n","\n","**Items**\n","item_nbr, family, class, perishable\n","\n","**Holidays_events**\n","date, type, locale, locale_name, description, transferred\n","\n","**Stores**\n","store_nbr, city, state, type, cluster\n","\n","**Oil**\n","date, dcoilwtico\n","\n","**Transactions**\n","date, store_nbr, transactions"]},{"attachments":{},"cell_type":"markdown","id":"b8ac14ae","metadata":{"id":"b8ac14ae","papermill":{"duration":0.168723,"end_time":"2022-01-15T09:54:04.231620","exception":false,"start_time":"2022-01-15T09:54:04.062897","status":"completed"},"tags":[]},"source":["**Selected features as inputs to the model**\n","\n","date, holiday.type, holidaye.locale, holiday.locale_name, holiday_transfered, store_nbr, store.city, store.state, store.type, store.cluster, transactions, item_nbr, item.family, item.class, on_promotion, perishable, dcoilwtico.\n","\n","**Selected features as outputs of the model**\n","\n","transactions per store, unit_sales per item"]},{"attachments":{},"cell_type":"markdown","id":"53bd3fc6","metadata":{"id":"53bd3fc6","papermill":{"duration":0.167221,"end_time":"2022-01-15T09:54:04.567001","exception":false,"start_time":"2022-01-15T09:54:04.399780","status":"completed"},"tags":[]},"source":["## DATA pipeline\n","\n","#### The next four cells are functions used as part of our Data Pipeline process.\n","#### Nothing Vectice specific, just boiler plate code.\n","#### Feel free to look through it but no need to spend time on it.\n","#### Go ahead and jump ahead to \"Documeetn in Vection\" section below"]},{"cell_type":"code","execution_count":null,"id":"9fa097ca","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1683296632258,"user":{"displayName":"Eric Barre","userId":"02191998857167728142"},"user_tz":240},"id":"9fa097ca","outputId":"c1f5b129-ffb9-45df-d3b7-d46fd0b987ff","papermill":{"duration":0.279687,"end_time":"2022-01-15T09:54:05.347688","exception":false,"start_time":"2022-01-15T09:54:05.068001","status":"completed"},"tags":[]},"outputs":[],"source":["class prepare_data(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        print(\"prepare_data -> init\")\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X, y=None):\n","        train_stores = X[0].merge(X[1], right_on = 'store_nbr', left_on='store_nbr')\n","        train_stores_oil = train_stores.merge(X[2], right_on='date', left_on='date')\n","        train_stores_oil_items = train_stores_oil.merge(X[3], right_on = 'item_nbr', left_on = 'item_nbr')\n","        train_stores_oil_items_transactions = train_stores_oil_items.merge(X[4], right_on = ['date', 'store_nbr'], left_on = ['date', 'store_nbr'])\n","        train_stores_oil_items_transactions_hol = train_stores_oil_items_transactions.merge(X[5], right_on = 'date', left_on = 'date')\n","        \n","        data_df = train_stores_oil_items_transactions_hol.copy(deep = True)\n","        \n","        # Fill the empty values\n","        data_df['onpromotion'] = data_df['onpromotion'].fillna(0)\n","        # change the bool to int\n","        data_df['onpromotion'] = data_df['onpromotion'].astype(int)\n","        data_df['transferred'] = data_df['transferred'].astype(int)\n","\n","        # change the names\n","        data_df.rename(columns={'type_x': 'st_type', 'type_y': 'hol_type'}, inplace=True)\n","        \n","        # handle date\n","        data_df['date'] = pd.to_datetime(data_df['date'])\n","        data_df['date'] = data_df['date'].map(dt.datetime.toordinal)\n","                \n","        return data_df"]},{"attachments":{},"cell_type":"markdown","id":"4770cb9a","metadata":{"id":"4770cb9a","papermill":{"duration":0.167427,"end_time":"2022-01-15T09:54:05.681875","exception":false,"start_time":"2022-01-15T09:54:05.514448","status":"completed"},"tags":[]},"source":["### Custom transform for splitting the data"]},{"attachments":{},"cell_type":"markdown","id":"c5f5bcfd","metadata":{"id":"c5f5bcfd"},"source":["Here, we split dataframe into numerical values, categorical values and date"]},{"cell_type":"code","execution_count":null,"id":"fba69204","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":335,"status":"ok","timestamp":1683296632589,"user":{"displayName":"Eric Barre","userId":"02191998857167728142"},"user_tz":240},"id":"fba69204","outputId":"07576068-3e16-4fe0-e9a1-7f2c240a4bdf","papermill":{"duration":0.177586,"end_time":"2022-01-15T09:54:06.025656","exception":false,"start_time":"2022-01-15T09:54:05.848070","status":"completed"},"tags":[]},"outputs":[],"source":["# split dataframe into numerical values, categorical values and date\n","class split_data(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        print(\"split_data -> init\")\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X, y=None):\n","        # Get columns for each type         \n","        df_ = X.drop(['date'], axis = 1)\n","        cols = df_.columns\n","        num_cols = df_._get_numeric_data().columns\n","        cat_cols = list(set(cols) - set(num_cols))\n","        \n","        data_num_df = X[num_cols]\n","        data_cat_df = X[cat_cols]\n","        data_date_df = X['date']\n","        \n","        return data_num_df, data_cat_df, data_date_df"]},{"attachments":{},"cell_type":"markdown","id":"7e98b316","metadata":{"id":"7e98b316","papermill":{"duration":0.165333,"end_time":"2022-01-15T09:54:06.357635","exception":false,"start_time":"2022-01-15T09:54:06.192302","status":"completed"},"tags":[]},"source":["Here, we handle the missing data, apply standard scaler to numerical attributes, and convert categorical data into numerical"]},{"cell_type":"code","execution_count":null,"id":"b3973ad6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1683296632590,"user":{"displayName":"Eric Barre","userId":"02191998857167728142"},"user_tz":240},"id":"b3973ad6","outputId":"e89fa911-21fc-470d-a2d8-411ee64480f8","papermill":{"duration":0.41475,"end_time":"2022-01-15T09:54:06.937863","exception":false,"start_time":"2022-01-15T09:54:06.523113","status":"completed"},"tags":[]},"outputs":[],"source":["class process_data(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        print(\"process_data -> init\")\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X, y=None):\n","        ### numerical data\n","        # impute nulls in numerical attributes\n","        imputer = SimpleImputer(strategy=\"mean\", copy=True)\n","        num_imp = imputer.fit_transform(X[0])\n","        #########\n","        data_num_df = pd.DataFrame(num_imp, columns=X[0].columns, index=X[0].index)\n","        \n","        # apply standard scaling\n","        scaler = StandardScaler()\n","        scaler.fit(data_num_df)\n","        num_scaled = scaler.transform(data_num_df)\n","        data_num_df = pd.DataFrame(num_scaled, columns=X[0].columns, index=X[0].index)\n","        \n","        ### categorical data\n","        # one hot encoder\n","        cat_encoder = OneHotEncoder(sparse=False)\n","        data_cat_1hot = cat_encoder.fit_transform(X[1])\n","        \n","        # convert it to dataframe with n*99 where n number of rows and 99 is no. of categories\n","        data_cat_df = pd.DataFrame(data_cat_1hot, columns=cat_encoder.get_feature_names_out()) #, index=X[1].index)\n","                \n","        return data_num_df, data_cat_df, X[2]"]},{"cell_type":"code","execution_count":null,"id":"83573257","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1683296632591,"user":{"displayName":"Eric Barre","userId":"02191998857167728142"},"user_tz":240},"id":"83573257","outputId":"edf571ee-8136-4752-8cf4-7eb07674d66f","papermill":{"duration":0.176369,"end_time":"2022-01-15T09:54:07.611183","exception":false,"start_time":"2022-01-15T09:54:07.434814","status":"completed"},"tags":[]},"outputs":[],"source":["class join_df(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        print(\"join_df -> init\")\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X, y=None):\n","        ### numerical data\n","        data_df = X[0].join(X[1])\n","        data_df = data_df.join(X[2])\n","        \n","        return data_df"]},{"attachments":{},"cell_type":"markdown","id":"61765158","metadata":{"id":"61765158"},"source":["# Push the datasets through the pipeline"]},{"cell_type":"code","execution_count":null,"id":"82d99b02","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66501,"status":"ok","timestamp":1683296699080,"user":{"displayName":"Eric Barre","userId":"02191998857167728142"},"user_tz":240},"id":"82d99b02","outputId":"2ae55fbb-055e-4c84-a6f6-3124937762bd"},"outputs":[],"source":["pipe_processing = Pipeline([\n","        ('prepare_data', prepare_data()),\n","        ('split_data', split_data()),\n","        ('process_data', process_data()),\n","        ('join_data', join_df())\n","    ])\n","\n","# our prepared data\n","data_df = pipe_processing.fit_transform([train, stores, oil, items, transactions, holiday_events])\n","data_df.to_csv(\"train_clean.csv\") #this is the dataset that will be split into a training, testing, and validation dataset"]},{"attachments":{},"cell_type":"markdown","id":"b420ffd1","metadata":{"id":"b420ffd1"},"source":["#### Navigate your way to your personal workspace, get the tutorial project and start an iteration of the 'Data Prep' phase. Go ahead and execute the cell below to navigate to your workspace. "]},{"cell_type":"code","execution_count":null,"id":"e16536bf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":404,"status":"ok","timestamp":1683296699466,"user":{"displayName":"Eric Barre","userId":"02191998857167728142"},"user_tz":240},"id":"e16536bf","outputId":"888799bf-bbee-44f5-a466-a3be754a5e56"},"outputs":[],"source":["active_iter = vec.phase(phs_id).create_iteration()"]},{"attachments":{},"cell_type":"markdown","id":"c3464f9a","metadata":{"id":"c3464f9a","tags":[]},"source":["## Capture milestones for the Data Preparation phase\n","\n","#### Execute the following cell"]},{"cell_type":"code","execution_count":null,"id":"984f0655","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4215,"status":"ok","timestamp":1683296703678,"user":{"displayName":"Eric Barre","userId":"02191998857167728142"},"user_tz":240},"id":"984f0655","outputId":"9f978102-4a94-4c2a-8182-5bd72166a1da"},"outputs":[],"source":["from vectice import FileResource\n","\n","# Provide context into the origin datasets by attaching them to the step\n","itm_ori = vct.Dataset.origin(name=\"Items origin\",resource=FileResource(paths=\"items.csv\", dataframes= items))\n","active_iter.step_select_data += itm_ori\n","hol_ori = vct.Dataset.origin(name=\"Holiday origin\",resource=FileResource(paths=\"holidays_events.csv\", dataframes= holiday_events))\n","active_iter.step_select_data += hol_ori\n","str_ori = vct.Dataset.origin(name=\"Stores origin\",resource=FileResource(paths=\"stores.csv\", dataframes=stores))\n","active_iter.step_select_data += str_ori\n","oil_ori = vct.Dataset.origin(name=\"Oil origin\",resource=FileResource(paths=\"oil.csv\", dataframes= oil))\n","active_iter.step_select_data += oil_ori\n","txs_ori = vct.Dataset.origin(name=\"Transactions origin\",resource=FileResource(paths=\"transactions.csv\", dataframes= transactions))\n","active_iter.step_select_data += txs_ori\n","\n","active_iter.step_select_data += \"The datasets for the project have been identified\"\n","\n","# Great we have documented the datasets used.\n","\n","# Let's move on the next step...documenting our data pipeline\n","# Log in findings/comments for this milestone\n","msg = \"As part of our standard Data Pipeline process we applied the following preparation to our datasets:\\n - Handling of missing data\\n - Applied standard scaler to numerical attributes\\n - Converted categorical data into numerical\\n - Split values in numerical values, categorical values, and dates\"\n","active_iter.step_clean_data += msg\n","\n","# Log in findings/comments for this milestone, close the step and capture the next one\n","active_iter.step_construct_data += \"We selected \\\"unit sales\\\" as our model target.\\nThe features used in this model are:\\n - date\\n - holiday.type\\n - holidaye.locale\\n - holiday.locale_name\\n - holiday_transfered\\n - store_nbr\\n - store.city\\n - store.state\\n - store.type\\n - store.cluster\\n - transactions\\n - item_nbr\\n - item.family\\n - item.class\\n - on_promotion\\n - perishable\\n - dcoilwtico\"\n","\n","# Log in findings/comments for this milestone, close the step and capture the next one'\n","# and attach the clean dataset generated\n","msg = \"We processed our origin datasets through our data pipeline to generate a dataset ready for modeling.\\n\"\n","msg += f\"The resulting modeling datasets contains {data_df.shape[0]} observations and {data_df.shape[1]} features.\\n\"\n","msg += \"The dataset is ready to be split for modeling.\"\n","active_iter.step_integrate_data += vct.Dataset.clean(name=\"Clean&Augmented_Dataset\",resource=FileResource(paths=\"train_clean.csv\", dataframes=data_df), \n","derived_from=[txs_ori.latest_version_id,oil_ori.latest_version_id,str_ori.latest_version_id,hol_ori.latest_version_id,itm_ori.latest_version_id])\n","active_iter.step_integrate_data += msg\n","\n","# Log in our activity\n","active_iter.step_format_data += \"We generated a dataset ready for modeling. We also created a data pipeline to make this process repeatable.\"\n","active_iter.complete()"]}],"metadata":{"colab":{"provenance":[]},"environment":{"kernel":"python3","name":"common-cpu.m94","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/base-cpu:m94"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"papermill":{"default_parameters":{},"duration":4527.062431,"end_time":"2022-01-15T11:06:02.218444","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-01-15T09:50:35.156013","version":"2.3.3"},"vscode":{"interpreter":{"hash":"397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"}}},"nbformat":4,"nbformat_minor":5}
